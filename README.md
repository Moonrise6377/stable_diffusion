# Readme
## Содержание
- [Задача](task1)
- [Описание подхода](task2)
- [Данные](task3)
- [Результаты](task4)
- [Что могли бы улучшить](task5)

## Задача <a class="anchor" id="task1"></a>
В рамках курса по NLP сайта ODS.ai, необходимо подготовить финальный проект по обработке текста. Необходимо подготовить отчет по примеру, предоставленному в рамках курса, описать свой подход, 
как были собраны данные, описать использованный подход. 
[Ссылка на курс](https://ods.ai/tracks/nlp-course-spring-23)

## Описание подхода <a class="anchor" id="task2"></a>
В качестве основной модели использовалась предобученная модель BLIP (Bootstraped Language-Imaged Pre-trained). Основное отличие модели заключается в том, что модель использует 
детектор изображений CapFilt для описания изображения. Готовить датасеты с описаниями изображений полностью с помощью человеческого труда дорого. Поэтому здесь нам и может помочь CapFilt.
Для удешевления сбора датасета данные собираются из интернета, с помощью парсинга собираются пары изображение-текст. CapFilt же работает следующим образом: модель генерирует описание изображения, 
а затем сравнивает сгенерированное описание с тем что было собрано из интернета и отбрасывает наименее релевантное. Это позволяет очистить данные от шума. Фактически мы используем еще одну 
нейронную сеть для того чтобы очистить данные от шума.  
Кроме использования предобученной модели BLIP мы также использовали и другие модели для сравнения результатов и предсказательной способности. Были использованы BLIP, ResNet & LSTM, CapDec, ClipCapm, VLKD.

### Метрика
В качестве метрик использовалась метрика BLEU.

## Данные <a class="anchor" id="task3"></a>
В проекте мы используем набор данных COCO за 2014 год. Набор данных доступен для скачивания на официальном веб-сайте. Набор данных COCO (Common Objects in Context) - это широко используемый набор
эталонных данных для задач обнаружения объектов, сегментации и создания субтитров. Мы сосредоточимся на части этого набора данных, содержащей подписи к изображениям. Набор данных содержит пять 
созданных человеком подписей к каждому изображению в наборах train, val и test.

## Результаты <a class="anchor" id="task4"></a>
Результаты всех моделей представлены ниже:
| Model         | BLEU-score |
|---------------|------------|
| BLIP          | 0.36       |
| ResNet & LSTM | 0.21       |
| CapDec        | 0.26       |
| ClipCap       | 0.32       |
| VLKD          | 0.17       |

## Что могли бы улучшить <a class="anchor" id="task5"></a>
Модель не была файтюнена под датасет. Можно добиться лучших результатов, улучшить метрику в случае, если мы попробуем подобрать параметры модели. Также уже существует модель BLIP-2, можно
попробовать применить ее и посмотреть на фактические различия в предсказательной способности моделей между версиями.
