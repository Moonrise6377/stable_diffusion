# -*- coding: utf-8 -*-
"""BLIP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jxVEWG2ynnsYLmO2_8fGbseS8eXKbbMm

Downloading val images
"""

import locale
locale.getpreferredencoding = lambda: "UTF-8"

!wget -P /content/ http://images.cocodataset.org/zips/val2014.zip

!unzip /content/val2014.zip -d /content/

"""Downloading annotations"""

!wget -P /content/ http://images.cocodataset.org/annotations/annotations_trainval2014.zip

!unzip /content/annotations_trainval2014.zip -d /content/

!pip install transformers

!pip install accelerate

import os
import sys
from pycocotools.coco import COCO

# path to cocoapi directory
cocoapi_dir = r"/content"
dir_type = "val2014"
# initialize COCO API for instance annotations
instances_ann_file = os.path.join(
    cocoapi_dir, "annotations", f"instances_{dir_type}.json"
)
coco = COCO(instances_ann_file)

# initialize COCO API for caption annotations
captions_ann_file = os.path.join(cocoapi_dir, "annotations", f"captions_{dir_type}.json")
coco_caps = COCO(captions_ann_file)

# get image ids
ids = list(coco.anns.keys())

from PIL import Image
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from nltk.translate.bleu_score import corpus_bleu
from pycocotools.coco import COCO
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")
model.to(device)

pred_imgs = []
true_caps = []
for i in ids:
  img_id = str(coco.anns[i]["image_id"])
  image_id = coco.anns[i]["image_id"]
  ann_id = coco.loadImgs(image_id)[0]['id']
  ann_ids = coco_caps.getAnnIds(imgIds=ann_id)
  true_caps.append(list(map(lambda ann: ann['caption'], coco_caps.loadAnns(ann_ids))))
  full_id = "0"*(12 - len(img_id)) + img_id
  path = f"/content/val2014/COCO_val2014_{full_id}.jpg"
  image = Image.open(path)
  pred_imgs.append(image)

generated_caps = []
for i in range(0, len(ids), 332):
  generated_ids = model.generate(**processor(images=pred_imgs[i:i+332], return_tensors="pt").to(device))
  generated_caps += processor.batch_decode(generated_ids, skip_special_tokens=True)
pred_caps = list(map(lambda x: x.strip(), generated_caps))

def bleu_score_blip(true_sentences, predicted_sentences):
    hypotheses = []
    references = []
    n = len(predicted_sentences)
    for i in range(0, n):
      img_refs = [cap.split() for cap in true_sentences[i]]
      references.append(img_refs)
      hypotheses.append(predicted_sentences[i].strip().split())

    return corpus_bleu(references, hypotheses)

bleu_score_blip(true_caps, pred_caps)

import matplotlib.pyplot as plt

j=0

for i in ids[:10]:
  img_id = str(coco.anns[i]["image_id"])
  full_id = "0"*(12 - len(img_id)) + img_id
  path = f"/content/val2014/COCO_val2014_{full_id}.jpg"
  image = Image.open(path)
  plt.axis("off")
  plt.imshow(image)
  plt.show()
  print(pred_caps[j])
  j+=1